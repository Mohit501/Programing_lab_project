<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<link href='https://fonts.googleapis.com/css?family=Open Sans' rel='stylesheet'>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Model Development</title>
	<style type="text/css">
		@media(min-width: 600px){
		body
		{
			font-family: 'Open Sans'
		}
		#Bigcontainer
		{
			background-color: #F8F9F9;
			display: grid;
			grid-template-columns: 250px auto
		}
		.container1
		{
			background-color: #17202A;
			font-size: 25px;
			color: whitesmoke;
			text-align: center;
			padding: 20px;
		}
		.container2
		{
			padding: 25px;
			
		}
		a
		{
			text-decoration: none;
			color: whitesmoke;
		}
		#lnk
		{
			text-decoration: none;
			color: cadetblue;
		}
		img
		    {
		    	height: 350px;
		    	width: 1000px;
		    }
	    }
	    @media(max-width: 599px){
	    body
		{
			font-family: 'Open Sans';
			font-size: 15px;
			background-color: darkred;
		}
		#Bigcontainer
		{
			background-color: greenyellow;
			display: grid;
			grid-template-columns: 120px auto
		}
		.container1
		{
			background-color: #17202A;
			color: whitesmoke;
			text-align: center;
		}
		.container2
		{
			padding: 25px;
			
		}
		a
		{
			text-decoration: none;
			color: whitesmoke;
		}
			
		}
	</style>
</head>
<body>
	<div id ="Bigcontainer">
	<div class="container1">

		<p><a href="{{url_for('About')}}">About</a></p>
		<p><a href="{{url_for('DataAnalysis')}}">Data Analysis</a></p>
		<p><a href="{{url_for('ModelDevelopment')}}">Model Development</a></p>
		<p><a href="{{url_for('PredictiveModel')}}">Predictive Model</a></p>
	</div>
	<div class="container2">
		<h1><center>Model Development</center></h1>
		<h2>1. Problem with the data </h2>
		<p> The classes in the data are imbalanced. That is in the dataset that we used the number of entries of chances of not having stroke were more than the entries of chances of having a stroke. This is called class imbalanced. This results in a machine learning model being highly accurate on the majority class and highly inacurate on the minority class.</p>
		<p> To solve this problem we used <em> Synthetic Minority Oversampling Technique (SMOTE)</em>.</p>
		<p>Synthetic Minority Oversampling Technique works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbor</p>
		<img src="{{url_for('static',filename='Screenshot 2022-11-15 at 4.10.31 PM.png')}}">
		<p>SMOTE algorithm works in 4 simple steps:
		<ol>
			<li>Choose a minority class as the input vector</li>
            <li>Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)</li>
            <li>Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor</li>
            <li>Repeat the steps until data is balanced</li></ol></p>
        <h2>2. Model Selection </h2>
        <p> The following models were ussed during model selection
        <ol>
        <li>Logistic Regression</li>
        <li>KNeighbor vClassifier</li>
        <li>Decision Tree Classifier</li>
        <li>Random Forest Classifier</li>
        <li>AdaBoost Classifier</li>
        <li>XGBOOST</li>
        </ol></p>
        <p> In order to select the best model the following metrics were used </p>
        <p><ol>
        	<li>Precision Score</li>
        	<li>Recall Score</li>
        	<li>Accuracy Score</li>
        	<li>Cross-Validation Score</li>
        </ol>
        </p>
        <p> Here are the results for each of the model </p>
        <table>
        	<tr>
        		<td> Model Name </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> Precision Score </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> Recall Score </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> Accuracy Score (Insample) </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> Accuracy Score (Outsample) </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> Cross-Val-Score(Insample)</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> Cross-Val-Score(Outsample)</td>
        	</tr>
        	<tr>
        		<td> Logistic Regression </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.7891</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.81</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.7977 </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.93</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.78</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.93</td>
        	</tr>
        	<tr>
        		<td> KNN </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.81</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.97</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.87 </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.93</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.87</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.93</td>
        	</tr>
        	<tr>
        		<td> Decision Tree </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.83</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.93</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.87 </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.91</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.87</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.90</td>
        	</tr>
        	<tr>
        		<td> Random Forest </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.9174</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.9571</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.9354 </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.9388</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.9372</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.9388</td>
        	</tr>
        	<tr>
        		<td> AdaBoost </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.87126</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.8835</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.8765 </td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.9192</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.8765</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.9192</td>
        	</tr>
        	<tr>
        		<td>XGBOOST</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.9577</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.9546</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.956</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.9143</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td> 0.95627</td>
        		<td>&nbsp</td>
        		<td>&nbsp</td>
        		<td>0.9143</td>
        	</tr>

        </table>
    <h2>3. Model Development</h2>
    <p> We have used Random Forest Classifier as our machine learning model. The tuned hyperparameters of the model are as follows:
    	<em>RandomForestClassifier(n_estimators = 200,n_jobs = -1,min_samples_split = 2,min_samples_leaf = 2)</em></p>
    <p> The following are the features the model captures for predictions: </p>
    <img src="{{url_for('static',filename='Screenshot 2022-11-15 at 4.33.39 PM.png')}}">




    </div>

</body>
</html>